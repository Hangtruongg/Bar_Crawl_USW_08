import os
import pickle
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import bc_data as bc  
from models.non_sequential import FeedForwardNet, Conv1DNet

# -----------------------------
# Settings
# -----------------------------
batch_size = 32
num_epochs = 20
seq_size = 100
learning_rate = 0.001
input_size = seq_size * 3

os.makedirs('plots_nonseq', exist_ok=True)
os.makedirs('results_nonseq', exist_ok=True)

# -----------------------------
# Dataset
# -----------------------------
dataset = bc.BarCrawlDataset(seq_size)
X = torch.stack([inputs.flatten() for inputs, label in dataset]).float()
y = torch.tensor([label for inputs, label in dataset], dtype=torch.float32).view(-1,1)

# Normalize
X_mean, X_std = X.mean(0, keepdim=True), X.std(0, keepdim=True)+1e-6
X = (X - X_mean)/X_std

# Train/validation split
val_ratio = 0.2
val_size = int(len(X) * val_ratio)
train_size = len(X) - val_size
X_train, X_val = torch.split(X, [train_size, val_size])
y_train, y_val = torch.split(y, [train_size, val_size])

train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)
val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)

# -----------------------------
# Hyperparameters to test
# -----------------------------
model_classes = {
    'FeedForward': FeedForwardNet,
    'Conv1D': Conv1DNet
}

hidden_sizes_list = [[64,32], [32,16]]
activation_list = ['relu', 'tanh']
optimizers_list = ['Adam', 'RMSprop']  
dropout_list = [0.1, 0.2]  

best_configs = {}

# -----------------------------
# Training loop
# -----------------------------
for model_name, ModelClass in model_classes.items():
    for hidden_sizes in hidden_sizes_list:
        for activation in activation_list:
            for optimizer_name in optimizers_list:
                for dropout in dropout_list:
                    
                    config_name = f"{model_name}_hidden{'-'.join(map(str,hidden_sizes))}_{activation}_{optimizer_name}_drop{dropout}"
                    print(f"\n--- Config: {config_name} ---")
                    
                    if model_name=='FeedForward':
                        model = FeedForwardNet(input_size=input_size, hidden_sizes=hidden_sizes,
                                               activation=activation, dropout=dropout)
                    else:
                        model = Conv1DNet(input_size=input_size, hidden_channels=hidden_sizes)
                    
                    # Adjust LR for SGD
                    lr = learning_rate if optimizer_name != 'SGD' else 0.0001
                    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=1e-4)
                    criterion = nn.MSELoss()
                    
                    train_loss_history = []
                    val_loss_history = []
                    
                    for epoch in range(num_epochs):
                        model.train()
                        running_train = 0.0
                        for inputs, labels in train_loader:
                            optimizer.zero_grad()
                            outputs = model(inputs)
                            loss = criterion(outputs, labels)
                            loss.backward()
                            optimizer.step()
                            running_train += loss.item()
                        train_loss = running_train / len(train_loader)
                        train_loss_history.append(train_loss)
                        
                        model.eval()
                        running_val = 0.0
                        with torch.no_grad():
                            for inputs, labels in val_loader:
                                outputs = model(inputs)
                                loss = criterion(outputs, labels)
                                running_val += loss.item()
                        val_loss = running_val / len(val_loader)
                        val_loss_history.append(val_loss)
                        
                        print(f"Epoch {epoch+1}/{num_epochs} - Train: {train_loss:.6f}, Val: {val_loss:.6f}")
                    
                    # Save plot
                    plt.figure(figsize=(8,5))
                    plt.plot(range(1,num_epochs+1), train_loss_history, marker='o', label='Train Loss')
                    plt.plot(range(1,num_epochs+1), val_loss_history, marker='s', label='Val Loss')
                    plt.xlabel("Epoch")
                    plt.ylabel("Loss")
                    plt.title(config_name)
                    plt.legend()
                    plt.grid(True)
                    plt.tight_layout()
                    plt.savefig(f"plots_nonseq/{config_name}.png")
                    plt.close()
                    
                    # Save loss history
                    with open(f"results_nonseq/loss_{config_name}.pkl", 'wb') as f:
                        pickle.dump({'train': train_loss_history, 'val': val_loss_history}, f)
                    
                    best_configs[config_name] = val_loss_history[-1]

# -----------------------------
# Best config
# -----------------------------
best_config_name = min(best_configs, key=best_configs.get)
print(f"\n=== BEST CONFIG ===\n{best_config_name} -> Val Loss: {best_configs[best_config_name]:.6f}")